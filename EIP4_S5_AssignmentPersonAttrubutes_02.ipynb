{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4-S5-AssignmentPersonAttrubutes-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarchandan/EIP4/blob/master/EIP4_S5_AssignmentPersonAttrubutes_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "img_rows, img_cols = 224, 224\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(images, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aHcHGaIJU68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.iloc[1370]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9F1CvBOJgUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "path = train_df.image_path[1370]\n",
        "Image(filename=path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "# Use some data augmentation\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df,\n",
        "    batch_size=32)\n",
        "\n",
        "valid_gen = PersonDataGenerator(\n",
        "    val_df,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHV0B30csWqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# backbone = VGG16(\n",
        "#     weights=\"imagenet\", \n",
        "#     include_top=False, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "backbone = Sequential()\n",
        "\n",
        "# 1st set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, activation='relu',  use_bias=False,input_shape=(224,224,3)))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 2nd set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 3rd set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 1, 1,use_bias=False, activation='relu'))\n",
        "backbone.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 4th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 5th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(16, 3, 3,use_bias=False, activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 6th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(16, 3, 3,use_bias=False, activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 7th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(16, 3, 3,use_bias=False, activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "# 8th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(10, 4, 4,use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "backbone.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 9th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "backbone.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 10th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "backbone.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 11th set of CONV-ReLU Layers\n",
        "backbone.add(Convolution2D(32, 3, 3, use_bias=False,activation='relu'))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Dropout(0.1))\n",
        "\n",
        "backbone.add(Convolution2D(10, 1,1, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "#\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  lr = 1e-3\n",
        "  if epoch > 150:\n",
        "    lr = round(0.1 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch > 110:\n",
        "    lr = round(0.1 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch > 20:\n",
        "    lr = 1e-3\n",
        "  elif epoch > 0:\n",
        "    lr = 1e-2\n",
        "  \n",
        "  print('Learning rate: ', lr)\n",
        "  return lr\n",
        "\n",
        "opt=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Checkpoints\n",
        "model_type='Assign5'\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "gender_filepath = os.path.join(save_dir, \"gender.h5\")\n",
        "imgQuality_filepath = os.path.join(save_dir, \"imgQuality.h5\")\n",
        "age_filepath = os.path.join(save_dir, \"age.h5\")\n",
        "weight_filepath = os.path.join(save_dir, \"weight.h5\")\n",
        "bag_filepath = os.path.join(save_dir, \"bag.h5\")\n",
        "footwear_filepath = os.path.join(save_dir, \"footwear.h5\")\n",
        "pose_filepath = os.path.join(save_dir, \"pose.h5\")\n",
        "emotion_filepath = os.path.join(save_dir, \"emotion.h5\")\n",
        "\n",
        "gender_checkpoint = ModelCheckpoint(filepath=gender_filepath,\n",
        "                             monitor='val_gender_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "imgQuality_checkpoint = ModelCheckpoint(filepath=imgQuality_filepath,\n",
        "                             monitor='val_image_quality_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "age_checkpoint = ModelCheckpoint(filepath=age_filepath,\n",
        "                             monitor='val_age_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "weight_checkpoint = ModelCheckpoint(filepath=weight_filepath,\n",
        "                             monitor='val_weight_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "bag_checkpoint = ModelCheckpoint(filepath=bag_filepath,\n",
        "                             monitor='val_bag_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "footwear_checkpoint = ModelCheckpoint(filepath=footwear_filepath,\n",
        "                             monitor='val_footwear_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "pose_checkpoint = ModelCheckpoint(filepath=pose_filepath,\n",
        "                             monitor='val_pose_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "emotion_checkpoint = ModelCheckpoint(filepath=emotion_filepath,\n",
        "                             monitor='val_emotion_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "#                               factor = 0.2,\n",
        "#                               patience = 3,\n",
        "#                               verbose = 1,\n",
        "#                               min_delta = 0.00001)\n",
        "\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler, verbose=1) #LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [earlystop, lr_reducer, lr_scheduler]\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIBxuqtOztu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}