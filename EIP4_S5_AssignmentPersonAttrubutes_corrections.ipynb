{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4-S5-AssignmentPersonAttrubutes-corrections.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarchandan/EIP4/blob/master/EIP4_S5_AssignmentPersonAttrubutes_corrections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "aecce5fd-d421-4e28-c1ad-014886a2fdcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        " # mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b213c184-69d6-4e4b-dbc6-74fb40be5818"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "82f2f4f6-e3c6-4f8d-aa2b-3ca86f225ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "7d199ee3-a35c-46be-a9f5-a45289054e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "img_rows, img_cols = 224, 224\n",
        "\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end() # can comment if we don't want to shuffle it for the first time\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        # return int(np.floor(self.df.shape[0] / self.batch_size)) // floor leaves some images/data, can be used in another batch, use ceil instead\n",
        "        return int(np.ceil(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(images, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "f116bfb4-ac1e-419a-f6fb-81b70810ba7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10858, 28), (2715, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "9a6bf332-500e-404f-a528-120fddaa4f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9909</th>\n",
              "      <td>resized/9910.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2181</th>\n",
              "      <td>resized/2182.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8589</th>\n",
              "      <td>resized/8590.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11210</th>\n",
              "      <td>resized/11212.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8799</th>\n",
              "      <td>resized/8800.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "9909    resized/9910.jpg              1  ...                        1              0\n",
              "2181    resized/2182.jpg              0  ...                        1              0\n",
              "8589    resized/8590.jpg              1  ...                        1              0\n",
              "11210  resized/11212.jpg              0  ...                        0              0\n",
              "8799    resized/8800.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aHcHGaIJU68",
        "colab_type": "code",
        "outputId": "2e2679e7-b801-4b8c-ae77-35e9fdf47045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "train_df.iloc[4675]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "image_path                              resized/11526.jpg\n",
              "gender_female                                           0\n",
              "gender_male                                             1\n",
              "imagequality_Average                                    0\n",
              "imagequality_Bad                                        1\n",
              "imagequality_Good                                       0\n",
              "age_15-25                                               0\n",
              "age_25-35                                               1\n",
              "age_35-45                                               0\n",
              "age_45-55                                               0\n",
              "age_55+                                                 0\n",
              "weight_normal-healthy                                   1\n",
              "weight_over-weight                                      0\n",
              "weight_slightly-overweight                              0\n",
              "weight_underweight                                      0\n",
              "carryingbag_Daily/Office/Work Bag                       0\n",
              "carryingbag_Grocery/Home/Plastic Bag                    0\n",
              "carryingbag_None                                        1\n",
              "footwear_CantSee                                        1\n",
              "footwear_Fancy                                          0\n",
              "footwear_Normal                                         0\n",
              "emotion_Angry/Serious                                   0\n",
              "emotion_Happy                                           0\n",
              "emotion_Neutral                                         1\n",
              "emotion_Sad                                             0\n",
              "bodypose_Back                                           0\n",
              "bodypose_Front-Frontish                                 1\n",
              "bodypose_Side                                           0\n",
              "Name: 11524, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9F1CvBOJgUy",
        "colab_type": "code",
        "outputId": "1ffd488c-5621-419e-b618-aae950a0a72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "path = train_df.image_path[4675]\n",
        "Image(filename=path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYF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\nO4Ad687F0+SSZ6+DrxlB8x+dVFFFUZBRRRQAUUUUAFFFFAH/2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "# Use some data augmentation\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df,\n",
        "    batch_size=32)\n",
        "\n",
        "valid_gen = PersonDataGenerator(\n",
        "    val_df,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "39798b4e-b076-4747-ed39-06108a04173c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHV0B30csWqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "34f07d6f-e5b0-4a4d-d54b-7fc37c866b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# backbone = VGG16(\n",
        "#     weights=\"imagenet\", \n",
        "#     include_top=False, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "backbone = Sequential()\n",
        "\n",
        "backbone.add(Convolution2D(16, (3, 3),dilation_rate=(2, 2),input_shape=(224, 224, 3))) #220\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(16, (3, 3),strides=(2,2))) #110\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(32, (3, 3),dilation_rate=(2, 2))) #106\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(32, 3, 3)) #104\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(48, (3, 3),strides=(2,2))) #52\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(48, (3, 3),dilation_rate=(2, 2))) #48\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "#backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(64, 3, 3)) #46\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(64, (3, 3),strides=(2,2))) #23\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(96, (3, 3),dilation_rate=(2, 2))) #19\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(96, (3, 3),strides=(2,2))) #9\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(Convolution2D(128, (3, 3),dilation_rate=(2, 2))) #5\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "backbone.add(GlobalAveragePooling2D(name='avg_pool'))\n",
        "\n",
        "\n",
        "neck = backbone.output\n",
        "# neck = Flatten(name=\"flatten\")(neck)\n",
        "# neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "#\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "conv2d_12_input (InputLayer)    (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 220, 220, 16) 448         conv2d_12_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 220, 220, 16) 64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 220, 220, 16) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 220, 220, 16) 0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 109, 109, 16) 2320        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 109, 109, 16) 64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 109, 109, 16) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 109, 109, 16) 0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 105, 105, 32) 4640        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 105, 105, 32) 128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 105, 105, 32) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 105, 105, 32) 0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 103, 103, 32) 9248        dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 103, 103, 32) 128         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 103, 103, 32) 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 103, 103, 32) 0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 51, 51, 48)   13872       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 51, 51, 48)   192         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 51, 51, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 51, 51, 48)   0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 47, 47, 48)   20784       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 47, 47, 48)   192         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 47, 47, 48)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 47, 47, 48)   0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 45, 45, 64)   27712       dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 45, 45, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 45, 45, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 45, 45, 64)   0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 22, 22, 64)   36928       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 22, 22, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 22, 22, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 22, 22, 64)   0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 18, 18, 96)   55392       dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 18, 18, 96)   384         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 18, 18, 96)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 18, 18, 96)   0           activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 96)     83040       dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 96)     384         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 96)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 8, 8, 96)     0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 128)    110720      dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 4, 4, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 4, 4, 128)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 128)          0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 128)          0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          16512       dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          16512       dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          16512       dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          16512       dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          16512       dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          16512       dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          16512       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          16512       dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 503,243\n",
            "Trainable params: 501,963\n",
            "Non-trainable params: 1,280\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  lr = 1e-3\n",
        "  if epoch > 150:\n",
        "    lr = round(0.1 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch > 110:\n",
        "    lr = round(0.1 * 1/(1 + 0.319 * epoch), 10)\n",
        "  elif epoch > 20:\n",
        "    lr = 1e-3\n",
        "  elif epoch > 0:\n",
        "    lr = 1e-2\n",
        "  \n",
        "  print('Learning rate: ', lr)\n",
        "  return lr\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "lr=0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Checkpoints\n",
        "model_type='Assign5'\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "gender_filepath = os.path.join(save_dir, \"gender.h5\")\n",
        "imgQuality_filepath = os.path.join(save_dir, \"imgQuality.h5\")\n",
        "age_filepath = os.path.join(save_dir, \"age.h5\")\n",
        "weight_filepath = os.path.join(save_dir, \"weight.h5\")\n",
        "bag_filepath = os.path.join(save_dir, \"bag.h5\")\n",
        "footwear_filepath = os.path.join(save_dir, \"footwear.h5\")\n",
        "pose_filepath = os.path.join(save_dir, \"pose.h5\")\n",
        "emotion_filepath = os.path.join(save_dir, \"emotion.h5\")\n",
        "\n",
        "gender_checkpoint = ModelCheckpoint(filepath=gender_filepath,\n",
        "                             monitor='val_gender_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "imgQuality_checkpoint = ModelCheckpoint(filepath=imgQuality_filepath,\n",
        "                             monitor='val_image_quality_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "age_checkpoint = ModelCheckpoint(filepath=age_filepath,\n",
        "                             monitor='val_age_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "weight_checkpoint = ModelCheckpoint(filepath=weight_filepath,\n",
        "                             monitor='val_weight_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "bag_checkpoint = ModelCheckpoint(filepath=bag_filepath,\n",
        "                             monitor='val_bag_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "footwear_checkpoint = ModelCheckpoint(filepath=footwear_filepath,\n",
        "                             monitor='val_footwear_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "pose_checkpoint = ModelCheckpoint(filepath=pose_filepath,\n",
        "                             monitor='val_pose_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "emotion_checkpoint = ModelCheckpoint(filepath=emotion_filepath,\n",
        "                             monitor='val_emotion_output_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "#                               factor = 0.2,\n",
        "#                               patience = 3,\n",
        "#                               verbose = 1,\n",
        "#                               min_delta = 0.00001)\n",
        "\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler, verbose=1) #LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler]\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIBxuqtOztu9",
        "colab_type": "code",
        "outputId": "05835b14-6daf-4e47-e2ad-4e4dbb33c114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=10,\n",
        "    callbacks=callbacks,\n",
        "    epochs=60,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 55s 162ms/step - loss: 7.8934 - gender_output_loss: 0.6718 - image_quality_output_loss: 0.9908 - age_output_loss: 1.4400 - weight_output_loss: 0.9946 - bag_output_loss: 0.9281 - footwear_output_loss: 0.9900 - pose_output_loss: 0.9447 - emotion_output_loss: 0.9333 - gender_output_acc: 0.5788 - image_quality_output_acc: 0.5468 - age_output_acc: 0.3866 - weight_output_acc: 0.6353 - bag_output_acc: 0.5500 - footwear_output_acc: 0.5151 - pose_output_acc: 0.6149 - emotion_output_acc: 0.7083 - val_loss: 8.3988 - val_gender_output_loss: 0.7435 - val_image_quality_output_loss: 1.1020 - val_age_output_loss: 1.4654 - val_weight_output_loss: 1.0684 - val_bag_output_loss: 1.0224 - val_footwear_output_loss: 1.0628 - val_pose_output_loss: 0.9273 - val_emotion_output_loss: 1.0072 - val_gender_output_acc: 0.5967 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.3989 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5624 - val_footwear_output_acc: 0.5462 - val_pose_output_acc: 0.6228 - val_emotion_output_acc: 0.7164\n",
            "Epoch 2/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 117ms/step - loss: 7.8350 - gender_output_loss: 0.6669 - image_quality_output_loss: 0.9860 - age_output_loss: 1.4305 - weight_output_loss: 0.9949 - bag_output_loss: 0.9225 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9373 - emotion_output_loss: 0.9257 - gender_output_acc: 0.5699 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3949 - weight_output_acc: 0.6368 - bag_output_acc: 0.5593 - footwear_output_acc: 0.5412 - pose_output_acc: 0.6152 - emotion_output_acc: 0.7105 - val_loss: 8.1338 - val_gender_output_loss: 0.7501 - val_image_quality_output_loss: 0.9995 - val_age_output_loss: 1.4358 - val_weight_output_loss: 1.0052 - val_bag_output_loss: 0.9547 - val_footwear_output_loss: 1.1302 - val_pose_output_loss: 0.9234 - val_emotion_output_loss: 0.9349 - val_gender_output_acc: 0.5669 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5120 - val_pose_output_acc: 0.6236 - val_emotion_output_acc: 0.7164\n",
            "Epoch 3/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.01.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 7.6998 - gender_output_loss: 0.6459 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4134 - weight_output_loss: 0.9794 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9367 - pose_output_loss: 0.9254 - emotion_output_loss: 0.9081 - gender_output_acc: 0.6085 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3987 - weight_output_acc: 0.6376 - bag_output_acc: 0.5619 - footwear_output_acc: 0.5659 - pose_output_acc: 0.6154 - emotion_output_acc: 0.7110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bLearning rate:  0.01\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.6995 - gender_output_loss: 0.6461 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4133 - weight_output_loss: 0.9787 - bag_output_loss: 0.9099 - footwear_output_loss: 0.9368 - pose_output_loss: 0.9248 - emotion_output_loss: 0.9087 - gender_output_acc: 0.6081 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3992 - weight_output_acc: 0.6379 - bag_output_acc: 0.5621 - footwear_output_acc: 0.5658 - pose_output_acc: 0.6159 - emotion_output_acc: 0.7107 - val_loss: 7.9602 - val_gender_output_loss: 0.7021 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4410 - val_weight_output_loss: 0.9956 - val_bag_output_loss: 0.9408 - val_footwear_output_loss: 1.0572 - val_pose_output_loss: 0.9087 - val_emotion_output_loss: 0.9295 - val_gender_output_acc: 0.5779 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3985 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.4530 - val_pose_output_acc: 0.6236 - val_emotion_output_acc: 0.7164\n",
            "Epoch 4/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.6335 - gender_output_loss: 0.6353 - image_quality_output_loss: 0.9767 - age_output_loss: 1.4128 - weight_output_loss: 0.9745 - bag_output_loss: 0.9018 - footwear_output_loss: 0.9151 - pose_output_loss: 0.9108 - emotion_output_loss: 0.9066 - gender_output_acc: 0.6205 - image_quality_output_acc: 0.5551 - age_output_acc: 0.3980 - weight_output_acc: 0.6380 - bag_output_acc: 0.5616 - footwear_output_acc: 0.5840 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7109 - val_loss: 7.8293 - val_gender_output_loss: 0.6956 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4291 - val_weight_output_loss: 0.9971 - val_bag_output_loss: 0.9443 - val_footwear_output_loss: 0.9867 - val_pose_output_loss: 0.8984 - val_emotion_output_loss: 0.8962 - val_gender_output_acc: 0.6320 - val_image_quality_output_acc: 0.5400 - val_age_output_acc: 0.3912 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.5613 - val_footwear_output_acc: 0.5808 - val_pose_output_acc: 0.6192 - val_emotion_output_acc: 0.7164\n",
            "\n",
            "Epoch 5/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 117ms/step - loss: 7.5804 - gender_output_loss: 0.6256 - image_quality_output_loss: 0.9763 - age_output_loss: 1.4099 - weight_output_loss: 0.9723 - bag_output_loss: 0.8973 - footwear_output_loss: 0.9045 - pose_output_loss: 0.8885 - emotion_output_loss: 0.9060 - gender_output_acc: 0.6388 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3971 - weight_output_acc: 0.6378 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6202 - emotion_output_acc: 0.7103 - val_loss: 7.6649 - val_gender_output_loss: 0.6636 - val_image_quality_output_loss: 1.0011 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.9105 - val_footwear_output_loss: 0.9102 - val_pose_output_loss: 0.8837 - val_emotion_output_loss: 0.8977 - val_gender_output_acc: 0.5937 - val_image_quality_output_acc: 0.5164 - val_age_output_acc: 0.3989 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5764 - val_pose_output_acc: 0.6221 - val_emotion_output_acc: 0.7164\n",
            "Epoch 6/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.01.\n",
            "\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 7.4858 - gender_output_loss: 0.6192 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4044 - weight_output_loss: 0.9665 - bag_output_loss: 0.8932 - footwear_output_loss: 0.8936 - pose_output_loss: 0.8326 - emotion_output_loss: 0.9008 - gender_output_acc: 0.6485 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3972 - weight_output_acc: 0.6377 - bag_output_acc: 0.5689 - footwear_output_acc: 0.5926 - pose_output_acc: 0.6366 - emotion_output_acc: 0.7103 - val_loss: 7.4946 - val_gender_output_loss: 0.6264 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8871 - val_footwear_output_loss: 0.9302 - val_pose_output_loss: 0.8077 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.6424 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5772 - val_footwear_output_acc: 0.5849 - val_pose_output_acc: 0.6586 - val_emotion_output_acc: 0.7164\n",
            "Epoch 7/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.3940 - gender_output_loss: 0.6077 - image_quality_output_loss: 0.9733 - age_output_loss: 1.4017 - weight_output_loss: 0.9640 - bag_output_loss: 0.8834 - footwear_output_loss: 0.8799 - pose_output_loss: 0.7892 - emotion_output_loss: 0.8949 - gender_output_acc: 0.6643 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3961 - weight_output_acc: 0.6383 - bag_output_acc: 0.5781 - footwear_output_acc: 0.5983 - pose_output_acc: 0.6545 - emotion_output_acc: 0.7107 - val_loss: 7.6222 - val_gender_output_loss: 0.6693 - val_image_quality_output_loss: 0.9883 - val_age_output_loss: 1.4231 - val_weight_output_loss: 0.9722 - val_bag_output_loss: 0.9306 - val_footwear_output_loss: 0.8787 - val_pose_output_loss: 0.8672 - val_emotion_output_loss: 0.8928 - val_gender_output_acc: 0.6424 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6254 - val_bag_output_acc: 0.5149 - val_footwear_output_acc: 0.6055 - val_pose_output_acc: 0.6564 - val_emotion_output_acc: 0.7164\n",
            "Epoch 8/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.3088 - gender_output_loss: 0.5959 - image_quality_output_loss: 0.9739 - age_output_loss: 1.4019 - weight_output_loss: 0.9604 - bag_output_loss: 0.8779 - footwear_output_loss: 0.8701 - pose_output_loss: 0.7375 - emotion_output_loss: 0.8912 - gender_output_acc: 0.6803 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3960 - weight_output_acc: 0.6383 - bag_output_acc: 0.5884 - footwear_output_acc: 0.5986 - pose_output_acc: 0.6836 - emotion_output_acc: 0.7101 - val_loss: 7.4280 - val_gender_output_loss: 0.6117 - val_image_quality_output_loss: 0.9843 - val_age_output_loss: 1.3895 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.9050 - val_footwear_output_loss: 0.9226 - val_pose_output_loss: 0.7634 - val_emotion_output_loss: 0.8764 - val_gender_output_acc: 0.6707 - val_image_quality_output_acc: 0.5436 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6254 - val_bag_output_acc: 0.5808 - val_footwear_output_acc: 0.5838 - val_pose_output_acc: 0.6600 - val_emotion_output_acc: 0.7164\n",
            "\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.01.Epoch 9/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.2211 - gender_output_loss: 0.5737 - image_quality_output_loss: 0.9738 - age_output_loss: 1.3952 - weight_output_loss: 0.9586 - bag_output_loss: 0.8731 - footwear_output_loss: 0.8579 - pose_output_loss: 0.7021 - emotion_output_loss: 0.8867 - gender_output_acc: 0.6975 - image_quality_output_acc: 0.5533 - age_output_acc: 0.3984 - weight_output_acc: 0.6375 - bag_output_acc: 0.5921 - footwear_output_acc: 0.6102 - pose_output_acc: 0.7031 - emotion_output_acc: 0.7110 - val_loss: 7.2862 - val_gender_output_loss: 0.5815 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.8758 - val_footwear_output_loss: 0.8869 - val_pose_output_loss: 0.7045 - val_emotion_output_loss: 0.8829 - val_gender_output_acc: 0.6980 - val_image_quality_output_acc: 0.5403 - val_age_output_acc: 0.3985 - val_weight_output_acc: 0.6177 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6004 - val_pose_output_acc: 0.6924 - val_emotion_output_acc: 0.7164\n",
            "Epoch 10/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.01.\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.1642 - gender_output_loss: 0.5465 - image_quality_output_loss: 0.9738 - age_output_loss: 1.3950 - weight_output_loss: 0.9591 - bag_output_loss: 0.8686 - footwear_output_loss: 0.8632 - pose_output_loss: 0.6734 - emotion_output_loss: 0.8846 - gender_output_acc: 0.7180 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3991 - weight_output_acc: 0.6396 - bag_output_acc: 0.5887 - footwear_output_acc: 0.6089 - pose_output_acc: 0.7131 - emotion_output_acc: 0.7106 - val_loss: 7.1428 - val_gender_output_loss: 0.5388 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.3817 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8771 - val_footwear_output_loss: 0.8589 - val_pose_output_loss: 0.6587 - val_emotion_output_loss: 0.8678 - val_gender_output_acc: 0.7193 - val_image_quality_output_acc: 0.5440 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.5834 - val_footwear_output_acc: 0.6155 - val_pose_output_acc: 0.7182 - val_emotion_output_acc: 0.7164\n",
            "Epoch 11/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 10/60\n",
            "Learning rate: \n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 7.0882 - gender_output_loss: 0.5184 - image_quality_output_loss: 0.9734 - age_output_loss: 1.3844 - weight_output_loss: 0.9538 - bag_output_loss: 0.8621 - footwear_output_loss: 0.8572 - pose_output_loss: 0.6534 - emotion_output_loss: 0.8855 - gender_output_acc: 0.7393 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4029 - weight_output_acc: 0.6401 - bag_output_acc: 0.6010 - footwear_output_acc: 0.6023 - pose_output_acc: 0.7269 - emotion_output_acc: 0.7107 - val_loss: 7.1763 - val_gender_output_loss: 0.5095 - val_image_quality_output_loss: 0.9894 - val_age_output_loss: 1.3882 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8709 - val_footwear_output_loss: 0.8554 - val_pose_output_loss: 0.7057 - val_emotion_output_loss: 0.8750 - val_gender_output_acc: 0.7392 - val_image_quality_output_acc: 0.5385 - val_age_output_acc: 0.3908 - val_weight_output_acc: 0.6243 - val_bag_output_acc: 0.5974 - val_footwear_output_acc: 0.6081 - val_pose_output_acc: 0.6796 - val_emotion_output_acc: 0.7157\n",
            "Epoch 12/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 7.0277 - gender_output_loss: 0.4933 - image_quality_output_loss: 0.9726 - age_output_loss: 1.3867 - weight_output_loss: 0.9529 - bag_output_loss: 0.8538 - footwear_output_loss: 0.8502 - pose_output_loss: 0.6388 - emotion_output_loss: 0.8793 - gender_output_acc: 0.7544 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3985 - weight_output_acc: 0.6399 - bag_output_acc: 0.6093 - footwear_output_acc: 0.6125 - pose_output_acc: 0.7297 - emotion_output_acc: 0.7101 - val_loss: 7.3754 - val_gender_output_loss: 0.5292 - val_image_quality_output_loss: 0.9878 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9695 - val_bag_output_loss: 0.8869 - val_footwear_output_loss: 1.0455 - val_pose_output_loss: 0.6739 - val_emotion_output_loss: 0.8737 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5304 - val_age_output_acc: 0.3709 - val_weight_output_acc: 0.6228 - val_bag_output_acc: 0.5941 - val_footwear_output_acc: 0.5109 - val_pose_output_acc: 0.7134 - val_emotion_output_acc: 0.7153\n",
            "Epoch 13/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.9486 - gender_output_loss: 0.4705 - image_quality_output_loss: 0.9727 - age_output_loss: 1.3826 - weight_output_loss: 0.9465 - bag_output_loss: 0.8492 - footwear_output_loss: 0.8393 - pose_output_loss: 0.6099 - emotion_output_loss: 0.8780 - gender_output_acc: 0.7717 - image_quality_output_acc: 0.5551 - age_output_acc: 0.3992 - weight_output_acc: 0.6410 - bag_output_acc: 0.6147 - footwear_output_acc: 0.6175 - pose_output_acc: 0.7508 - emotion_output_acc: 0.7107 - val_loss: 7.0594 - val_gender_output_loss: 0.5411 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.3912 - val_weight_output_loss: 0.9580 - val_bag_output_loss: 0.8809 - val_footwear_output_loss: 0.8395 - val_pose_output_loss: 0.5920 - val_emotion_output_loss: 0.8762 - val_gender_output_acc: 0.7554 - val_image_quality_output_acc: 0.5436 - val_age_output_acc: 0.4015 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.6041 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.7576 - val_emotion_output_acc: 0.7127\n",
            " 0.01\n",
            "Epoch 14/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.01.\n",
            "\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.9168 - gender_output_loss: 0.4564 - image_quality_output_loss: 0.9709 - age_output_loss: 1.3790 - weight_output_loss: 0.9464 - bag_output_loss: 0.8434 - footwear_output_loss: 0.8459 - pose_output_loss: 0.5958 - emotion_output_loss: 0.8790 - gender_output_acc: 0.7845 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4060 - weight_output_acc: 0.6424 - bag_output_acc: 0.6176 - footwear_output_acc: 0.6190 - pose_output_acc: 0.7518 - emotion_output_acc: 0.7102 - val_loss: 7.1090 - val_gender_output_loss: 0.5183 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8638 - val_footwear_output_loss: 0.8935 - val_pose_output_loss: 0.6317 - val_emotion_output_loss: 0.8670 - val_gender_output_acc: 0.7613 - val_image_quality_output_acc: 0.5455 - val_age_output_acc: 0.3956 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.5753 - val_pose_output_acc: 0.7201 - val_emotion_output_acc: 0.7164\n",
            "Epoch 15/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.8599 - gender_output_loss: 0.4399 - image_quality_output_loss: 0.9695 - age_output_loss: 1.3791 - weight_output_loss: 0.9438 - bag_output_loss: 0.8340 - footwear_output_loss: 0.8360 - pose_output_loss: 0.5849 - emotion_output_loss: 0.8727 - gender_output_acc: 0.7946 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4058 - weight_output_acc: 0.6434 - bag_output_acc: 0.6261 - footwear_output_acc: 0.6229 - pose_output_acc: 0.7620 - emotion_output_acc: 0.7102 - val_loss: 6.9566 - val_gender_output_loss: 0.4267 - val_image_quality_output_loss: 0.9825 - val_age_output_loss: 1.4042 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.8524 - val_footwear_output_loss: 0.8526 - val_pose_output_loss: 0.5863 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.8037 - val_image_quality_output_acc: 0.5455 - val_age_output_acc: 0.3912 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.6144 - val_footwear_output_acc: 0.6214 - val_pose_output_acc: 0.7576 - val_emotion_output_acc: 0.7164\n",
            "Epoch 16/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.01.\n",
            "\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 6.8074 - gender_output_loss: 0.4251 - image_quality_output_loss: 0.9679 - age_output_loss: 1.3728 - weight_output_loss: 0.9410 - bag_output_loss: 0.8282 - footwear_output_loss: 0.8343 - pose_output_loss: 0.5644 - emotion_output_loss: 0.8739 - gender_output_acc: 0.8037 - image_quality_output_acc: 0.5572 - age_output_acc: 0.4027 - weight_output_acc: 0.6415 - bag_output_acc: 0.6289 - footwear_output_acc: 0.6251 - pose_output_acc: 0.7684 - emotion_output_acc: 0.7105 - val_loss: 6.9968 - val_gender_output_loss: 0.4389 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9607 - val_bag_output_loss: 0.8609 - val_footwear_output_loss: 0.8396 - val_pose_output_loss: 0.6497 - val_emotion_output_loss: 0.8803 - val_gender_output_acc: 0.7974 - val_image_quality_output_acc: 0.5436 - val_age_output_acc: 0.3941 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6110 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.7109 - val_emotion_output_acc: 0.7164\n",
            "Epoch 17/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.7689 - gender_output_loss: 0.4204 - image_quality_output_loss: 0.9676 - age_output_loss: 1.3674 - weight_output_loss: 0.9364 - bag_output_loss: 0.8249 - footwear_output_loss: 0.8325 - pose_output_loss: 0.5516 - emotion_output_loss: 0.8681 - gender_output_acc: 0.8021 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4119 - weight_output_acc: 0.6443 - bag_output_acc: 0.6326 - footwear_output_acc: 0.6215 - pose_output_acc: 0.7748 - emotion_output_acc: 0.7108 - val_loss: 7.1023 - val_gender_output_loss: 0.4875 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.3903 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.9030 - val_footwear_output_loss: 0.8849 - val_pose_output_loss: 0.6087 - val_emotion_output_loss: 0.8727 - val_gender_output_acc: 0.7672 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.4059 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5772 - val_footwear_output_acc: 0.6199 - val_pose_output_acc: 0.7510 - val_emotion_output_acc: 0.7164\n",
            "Epoch 18/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.7095 - gender_output_loss: 0.4043 - image_quality_output_loss: 0.9656 - age_output_loss: 1.3642 - weight_output_loss: 0.9296 - bag_output_loss: 0.8221 - footwear_output_loss: 0.8217 - pose_output_loss: 0.5310 - emotion_output_loss: 0.8712 - gender_output_acc: 0.8163 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4053 - weight_output_acc: 0.6473 - bag_output_acc: 0.6348 - footwear_output_acc: 0.6296 - pose_output_acc: 0.7818 - emotion_output_acc: 0.7100 - val_loss: 6.7757 - val_gender_output_loss: 0.4056 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.3717 - val_weight_output_loss: 0.9477 - val_bag_output_loss: 0.8397 - val_footwear_output_loss: 0.8208 - val_pose_output_loss: 0.5477 - val_emotion_output_loss: 0.8645 - val_gender_output_acc: 0.8162 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.4206 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6221 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.7805 - val_emotion_output_acc: 0.7164\n",
            "Epoch 19/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.01.\n",
            "Learning rate:  0.01\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.6857 - gender_output_loss: 0.4011 - image_quality_output_loss: 0.9625 - age_output_loss: 1.3643 - weight_output_loss: 0.9311 - bag_output_loss: 0.8195 - footwear_output_loss: 0.8180 - pose_output_loss: 0.5265 - emotion_output_loss: 0.8628 - gender_output_acc: 0.8159 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4140 - weight_output_acc: 0.6430 - bag_output_acc: 0.6369 - footwear_output_acc: 0.6320 - pose_output_acc: 0.7863 - emotion_output_acc: 0.7109 - val_loss: 6.8527 - val_gender_output_loss: 0.4363 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.3756 - val_weight_output_loss: 0.9472 - val_bag_output_loss: 0.8748 - val_footwear_output_loss: 0.8424 - val_pose_output_loss: 0.5296 - val_emotion_output_loss: 0.8651 - val_gender_output_acc: 0.8004 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6328 - val_bag_output_acc: 0.5923 - val_footwear_output_acc: 0.6254 - val_pose_output_acc: 0.7901 - val_emotion_output_acc: 0.7160\n",
            "Epoch 20/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.01.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.6421 - gender_output_loss: 0.3932 - image_quality_output_loss: 0.9624 - age_output_loss: 1.3568 - weight_output_loss: 0.9258 - bag_output_loss: 0.8104 - footwear_output_loss: 0.8113 - pose_output_loss: 0.5146 - emotion_output_loss: 0.8677 - gender_output_acc: 0.8191 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4119 - weight_output_acc: 0.6480 - bag_output_acc: 0.6464 - footwear_output_acc: 0.6353 - pose_output_acc: 0.7960 - emotion_output_acc: 0.7105 - val_loss: 6.8045 - val_gender_output_loss: 0.4078 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.3790 - val_weight_output_loss: 0.9577 - val_bag_output_loss: 0.8313 - val_footwear_output_loss: 0.8429 - val_pose_output_loss: 0.5421 - val_emotion_output_loss: 0.8678 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.4122 - val_weight_output_acc: 0.6169 - val_bag_output_acc: 0.6254 - val_footwear_output_acc: 0.6147 - val_pose_output_acc: 0.7820 - val_emotion_output_acc: 0.7123\n",
            "Epoch 21/60\n",
            "Learning rate:  0.01\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.01.\n",
            "  1/340 [..............................] - ETA: 1:43 - loss: 5.8777 - gender_output_loss: 0.2807 - image_quality_output_loss: 0.9599 - age_output_loss: 1.3210 - weight_output_loss: 0.7706 - bag_output_loss: 0.6382 - footwear_output_loss: 0.8238 - pose_output_loss: 0.3562 - emotion_output_loss: 0.7273 - gender_output_acc: 0.9062 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4688 - weight_output_acc: 0.7500 - bag_output_acc: 0.7188 - footwear_output_acc: 0.5938 - pose_output_acc: 0.8438 - emotion_output_acc: 0.7500\n",
            "Epoch 20/60\n",
            "Learning rate:  0.01\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.6295 - gender_output_loss: 0.3831 - image_quality_output_loss: 0.9586 - age_output_loss: 1.3599 - weight_output_loss: 0.9303 - bag_output_loss: 0.8063 - footwear_output_loss: 0.8129 - pose_output_loss: 0.5142 - emotion_output_loss: 0.8643 - gender_output_acc: 0.8270 - image_quality_output_acc: 0.5570 - age_output_acc: 0.4119 - weight_output_acc: 0.6458 - bag_output_acc: 0.6547 - footwear_output_acc: 0.6353 - pose_output_acc: 0.7891 - emotion_output_acc: 0.7104 - val_loss: 6.8480 - val_gender_output_loss: 0.4314 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.3903 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8472 - val_footwear_output_loss: 0.8337 - val_pose_output_loss: 0.5222 - val_emotion_output_loss: 0.8621 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6287 - val_pose_output_acc: 0.7878 - val_emotion_output_acc: 0.7164\n",
            "Epoch 22/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.4223 - gender_output_loss: 0.3427 - image_quality_output_loss: 0.9465 - age_output_loss: 1.3406 - weight_output_loss: 0.9112 - bag_output_loss: 0.7829 - footwear_output_loss: 0.7871 - pose_output_loss: 0.4590 - emotion_output_loss: 0.8522 - gender_output_acc: 0.8498 - image_quality_output_acc: 0.5590 - age_output_acc: 0.4195 - weight_output_acc: 0.6507 - bag_output_acc: 0.6616 - footwear_output_acc: 0.6447 - pose_output_acc: 0.8198 - emotion_output_acc: 0.7110 - val_loss: 6.6399 - val_gender_output_loss: 0.3726 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.3632 - val_weight_output_loss: 0.9325 - val_bag_output_loss: 0.8213 - val_footwear_output_loss: 0.8146 - val_pose_output_loss: 0.4938 - val_emotion_output_loss: 0.8592 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.4107 - val_weight_output_acc: 0.6298 - val_bag_output_acc: 0.6387 - val_footwear_output_acc: 0.6379 - val_pose_output_acc: 0.8041 - val_emotion_output_acc: 0.7157\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.6295 - gender_output_loss: 0.3831 - image_quality_output_loss: 0.9586 - age_output_loss: 1.3599 - weight_output_loss: 0.9303 - bag_output_loss: 0.8063 - footwear_output_loss: 0.8129 - pose_output_loss: 0.5142 - emotion_output_loss: 0.8643 - gender_output_acc: 0.8270 - image_quality_output_acc: 0.5570 - age_output_acc: 0.4119 - weight_output_acc: 0.6458 - bag_output_acc: 0.6547 - footwear_output_acc: 0.6353 - pose_output_acc: 0.7891 - emotion_output_acc: 0.7104 - val_loss: 6.8480 - val_gender_output_loss: 0.4314 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.3903 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8472 - val_footwear_output_loss: 0.8337 - val_pose_output_loss: 0.5222 - val_emotion_output_loss: 0.8621 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6287 - val_pose_output_acc: 0.7878 - val_emotion_output_acc: 0.7164\n",
            "Epoch 23/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 6.3504 - gender_output_loss: 0.3273 - image_quality_output_loss: 0.9433 - age_output_loss: 1.3360 - weight_output_loss: 0.9017 - bag_output_loss: 0.7762 - footwear_output_loss: 0.7804 - pose_output_loss: 0.4349 - emotion_output_loss: 0.8507 - gender_output_acc: 0.8591 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4140 - weight_output_acc: 0.6502 - bag_output_acc: 0.6679 - footwear_output_acc: 0.6500 - pose_output_acc: 0.8329 - emotion_output_acc: 0.7105Epoch 23/60\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 6.3500 - gender_output_loss: 0.3277 - image_quality_output_loss: 0.9430 - age_output_loss: 1.3357 - weight_output_loss: 0.9019 - bag_output_loss: 0.7763 - footwear_output_loss: 0.7801 - pose_output_loss: 0.4345 - emotion_output_loss: 0.8509 - gender_output_acc: 0.8590 - image_quality_output_acc: 0.5620 - age_output_acc: 0.4140 - weight_output_acc: 0.6501 - bag_output_acc: 0.6679 - footwear_output_acc: 0.6500 - pose_output_acc: 0.8331 - emotion_output_acc: 0.7103 - val_loss: 6.5993 - val_gender_output_loss: 0.3647 - val_image_quality_output_loss: 0.9782 - val_age_output_loss: 1.3582 - val_weight_output_loss: 0.9293 - val_bag_output_loss: 0.8146 - val_footwear_output_loss: 0.8105 - val_pose_output_loss: 0.4867 - val_emotion_output_loss: 0.8571 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4122 - val_weight_output_acc: 0.6317 - val_bag_output_acc: 0.6401 - val_footwear_output_acc: 0.6424 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7160\n",
            "Epoch 24/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 6.3076 - gender_output_loss: 0.3124 - image_quality_output_loss: 0.9408 - age_output_loss: 1.3275 - weight_output_loss: 0.9043 - bag_output_loss: 0.7759 - footwear_output_loss: 0.7701 - pose_output_loss: 0.4256 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8676 - image_quality_output_acc: 0.5653 - age_output_acc: 0.4183 - weight_output_acc: 0.6482 - bag_output_acc: 0.6696 - footwear_output_acc: 0.6557 - pose_output_acc: 0.8326 - emotion_output_acc: 0.7104Epoch 24/60\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.3081 - gender_output_loss: 0.3121 - image_quality_output_loss: 0.9408 - age_output_loss: 1.3275 - weight_output_loss: 0.9037 - bag_output_loss: 0.7762 - footwear_output_loss: 0.7704 - pose_output_loss: 0.4265 - emotion_output_loss: 0.8507 - gender_output_acc: 0.8677 - image_quality_output_acc: 0.5651 - age_output_acc: 0.4186 - weight_output_acc: 0.6484 - bag_output_acc: 0.6695 - footwear_output_acc: 0.6555 - pose_output_acc: 0.8325 - emotion_output_acc: 0.7106 - val_loss: 6.6082 - val_gender_output_loss: 0.3620 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.3679 - val_weight_output_loss: 0.9356 - val_bag_output_loss: 0.8127 - val_footwear_output_loss: 0.8107 - val_pose_output_loss: 0.4813 - val_emotion_output_loss: 0.8585 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6217 - val_bag_output_acc: 0.6435 - val_footwear_output_acc: 0.6409 - val_pose_output_acc: 0.8066 - val_emotion_output_acc: 0.7157\n",
            "Epoch 25/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.2911 - gender_output_loss: 0.3168 - image_quality_output_loss: 0.9396 - age_output_loss: 1.3283 - weight_output_loss: 0.8969 - bag_output_loss: 0.7699 - footwear_output_loss: 0.7665 - pose_output_loss: 0.4246 - emotion_output_loss: 0.8485 - gender_output_acc: 0.8613 - image_quality_output_acc: 0.5614 - age_output_acc: 0.4170 - weight_output_acc: 0.6502 - bag_output_acc: 0.6711 - footwear_output_acc: 0.6607 - pose_output_acc: 0.8326 - emotion_output_acc: 0.7106 - val_loss: 6.5978 - val_gender_output_loss: 0.3598 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.3613 - val_weight_output_loss: 0.9304 - val_bag_output_loss: 0.8150 - val_footwear_output_loss: 0.8112 - val_pose_output_loss: 0.4822 - val_emotion_output_loss: 0.8585 - val_gender_output_acc: 0.8449 - val_image_quality_output_acc: 0.5462 - val_age_output_acc: 0.4144 - val_weight_output_acc: 0.6287 - val_bag_output_acc: 0.6468 - val_footwear_output_acc: 0.6427 - val_pose_output_acc: 0.8077 - val_emotion_output_acc: 0.7149\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.2689 - gender_output_loss: 0.3084 - image_quality_output_loss: 0.9366 - age_output_loss: 1.3239 - weight_output_loss: 0.8949 - bag_output_loss: 0.7654 - footwear_output_loss: 0.7645 - pose_output_loss: 0.4264 - emotion_output_loss: 0.8487 - gender_output_acc: 0.8692 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4233 - weight_output_acc: 0.6523 - bag_output_acc: 0.6730 - footwear_output_acc: 0.6605 - pose_output_acc: 0.8306 - emotion_output_acc: 0.7107 - val_loss: 6.6059 - val_gender_output_loss: 0.3665 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.3580 - val_weight_output_loss: 0.9270 - val_bag_output_loss: 0.8142 - val_footwear_output_loss: 0.8159 - val_pose_output_loss: 0.4809 - val_emotion_output_loss: 0.8585 - val_gender_output_acc: 0.8431 - val_image_quality_output_acc: 0.5414 - val_age_output_acc: 0.4140 - val_weight_output_acc: 0.6291 - val_bag_output_acc: 0.6435 - val_footwear_output_acc: 0.6398 - val_pose_output_acc: 0.8074 - val_emotion_output_acc: 0.7164\n",
            "Epoch 27/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.2525 - gender_output_loss: 0.3076 - image_quality_output_loss: 0.9396 - age_output_loss: 1.3251 - weight_output_loss: 0.8934 - bag_output_loss: 0.7676 - footwear_output_loss: 0.7601 - pose_output_loss: 0.4131 - emotion_output_loss: 0.8460 - gender_output_acc: 0.8716 - image_quality_output_acc: 0.5602 - age_output_acc: 0.4254 - weight_output_acc: 0.6551 - bag_output_acc: 0.6721 - footwear_output_acc: 0.6613 - pose_output_acc: 0.8419 - emotion_output_acc: 0.7109 - val_loss: 6.5795 - val_gender_output_loss: 0.3571 - val_image_quality_output_loss: 0.9747 - val_age_output_loss: 1.3542 - val_weight_output_loss: 0.9272 - val_bag_output_loss: 0.8131 - val_footwear_output_loss: 0.8107 - val_pose_output_loss: 0.4813 - val_emotion_output_loss: 0.8613 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5436 - val_age_output_acc: 0.4192 - val_weight_output_acc: 0.6298 - val_bag_output_acc: 0.6464 - val_footwear_output_acc: 0.6435 - val_pose_output_acc: 0.8052 - val_emotion_output_acc: 0.7164\n",
            "Epoch 28/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 6.2333 - gender_output_loss: 0.3025 - image_quality_output_loss: 0.9363 - age_output_loss: 1.3222 - weight_output_loss: 0.8937 - bag_output_loss: 0.7661 - footwear_output_loss: 0.7564 - pose_output_loss: 0.4094 - emotion_output_loss: 0.8466 - gender_output_acc: 0.8708 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4249 - weight_output_acc: 0.6566 - bag_output_acc: 0.6741 - footwear_output_acc: 0.6620 - pose_output_acc: 0.8421 - emotion_output_acc: 0.7109 - val_loss: 6.5928 - val_gender_output_loss: 0.3630 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.3571 - val_weight_output_loss: 0.9297 - val_bag_output_loss: 0.8112 - val_footwear_output_loss: 0.8104 - val_pose_output_loss: 0.4843 - val_emotion_output_loss: 0.8632 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.5473 - val_age_output_acc: 0.4162 - val_weight_output_acc: 0.6276 - val_bag_output_acc: 0.6460 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.8085 - val_emotion_output_acc: 0.7145\n",
            "Epoch 29/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.2124 - gender_output_loss: 0.3041 - image_quality_output_loss: 0.9352 - age_output_loss: 1.3196 - weight_output_loss: 0.8848 - bag_output_loss: 0.7640 - footwear_output_loss: 0.7543 - pose_output_loss: 0.4051 - emotion_output_loss: 0.8454 - gender_output_acc: 0.8680 - image_quality_output_acc: 0.5639 - age_output_acc: 0.4244 - weight_output_acc: 0.6586 - bag_output_acc: 0.6721 - footwear_output_acc: 0.6649 - pose_output_acc: 0.8446 - emotion_output_acc: 0.7112 - val_loss: 6.5828 - val_gender_output_loss: 0.3584 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.3573 - val_weight_output_loss: 0.9289 - val_bag_output_loss: 0.8098 - val_footwear_output_loss: 0.8129 - val_pose_output_loss: 0.4828 - val_emotion_output_loss: 0.8591 - val_gender_output_acc: 0.8457 - val_image_quality_output_acc: 0.5473 - val_age_output_acc: 0.4155 - val_weight_output_acc: 0.6262 - val_bag_output_acc: 0.6435 - val_footwear_output_acc: 0.6420 - val_pose_output_acc: 0.8048 - val_emotion_output_acc: 0.7153\n",
            "Epoch 30/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/60\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 6.2016 - gender_output_loss: 0.2971 - image_quality_output_loss: 0.9310 - age_output_loss: 1.3183 - weight_output_loss: 0.8864 - bag_output_loss: 0.7589 - footwear_output_loss: 0.7546 - pose_output_loss: 0.4094 - emotion_output_loss: 0.8458 - gender_output_acc: 0.8712 - image_quality_output_acc: 0.5649 - age_output_acc: 0.4256 - weight_output_acc: 0.6595 - bag_output_acc: 0.6797 - footwear_output_acc: 0.6627 - pose_output_acc: 0.8389 - emotion_output_acc: 0.7113 - val_loss: 6.6168 - val_gender_output_loss: 0.3677 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.3608 - val_weight_output_loss: 0.9331 - val_bag_output_loss: 0.8081 - val_footwear_output_loss: 0.8178 - val_pose_output_loss: 0.4868 - val_emotion_output_loss: 0.8611 - val_gender_output_acc: 0.8424 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.4107 - val_weight_output_acc: 0.6254 - val_bag_output_acc: 0.6490 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.8063 - val_emotion_output_acc: 0.7164\n",
            "Epoch 31/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.1826 - gender_output_loss: 0.2948 - image_quality_output_loss: 0.9336 - age_output_loss: 1.3199 - weight_output_loss: 0.8826 - bag_output_loss: 0.7570 - footwear_output_loss: 0.7524 - pose_output_loss: 0.3978 - emotion_output_loss: 0.8445 - gender_output_acc: 0.8754 - image_quality_output_acc: 0.5660 - age_output_acc: 0.4245 - weight_output_acc: 0.6582 - bag_output_acc: 0.6799 - footwear_output_acc: 0.6688 - pose_output_acc: 0.8426 - emotion_output_acc: 0.7106 - val_loss: 6.5684 - val_gender_output_loss: 0.3540 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.3580 - val_weight_output_loss: 0.9260 - val_bag_output_loss: 0.8069 - val_footwear_output_loss: 0.8115 - val_pose_output_loss: 0.4782 - val_emotion_output_loss: 0.8602 - val_gender_output_acc: 0.8464 - val_image_quality_output_acc: 0.5495 - val_age_output_acc: 0.4099 - val_weight_output_acc: 0.6258 - val_bag_output_acc: 0.6505 - val_footwear_output_acc: 0.6398 - val_pose_output_acc: 0.8085 - val_emotion_output_acc: 0.7157\n",
            "Epoch 32/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.1617 - gender_output_loss: 0.2866 - image_quality_output_loss: 0.9293 - age_output_loss: 1.3147 - weight_output_loss: 0.8803 - bag_output_loss: 0.7569 - footwear_output_loss: 0.7525 - pose_output_loss: 0.3971 - emotion_output_loss: 0.8444 - gender_output_acc: 0.8771 - image_quality_output_acc: 0.5688 - age_output_acc: 0.4275 - weight_output_acc: 0.6581 - bag_output_acc: 0.6794 - footwear_output_acc: 0.6659 - pose_output_acc: 0.8442 - emotion_output_acc: 0.7104 - val_loss: 6.5590 - val_gender_output_loss: 0.3541 - val_image_quality_output_loss: 0.9694 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9261 - val_bag_output_loss: 0.8092 - val_footwear_output_loss: 0.8077 - val_pose_output_loss: 0.4784 - val_emotion_output_loss: 0.8606 - val_gender_output_acc: 0.8457 - val_image_quality_output_acc: 0.5488 - val_age_output_acc: 0.4118 - val_weight_output_acc: 0.6291 - val_bag_output_acc: 0.6530 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.8107 - val_emotion_output_acc: 0.7157\n",
            "Learning rate:  0.001Epoch 33/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.1495 - gender_output_loss: 0.2868 - image_quality_output_loss: 0.9261 - age_output_loss: 1.3139 - weight_output_loss: 0.8810 - bag_output_loss: 0.7559 - footwear_output_loss: 0.7537 - pose_output_loss: 0.3908 - emotion_output_loss: 0.8414 - gender_output_acc: 0.8803 - image_quality_output_acc: 0.5653 - age_output_acc: 0.4279 - weight_output_acc: 0.6551 - bag_output_acc: 0.6826 - footwear_output_acc: 0.6657 - pose_output_acc: 0.8482 - emotion_output_acc: 0.7108 - val_loss: 6.5690 - val_gender_output_loss: 0.3539 - val_image_quality_output_loss: 0.9690 - val_age_output_loss: 1.3550 - val_weight_output_loss: 0.9265 - val_bag_output_loss: 0.8091 - val_footwear_output_loss: 0.8087 - val_pose_output_loss: 0.4855 - val_emotion_output_loss: 0.8614 - val_gender_output_acc: 0.8431 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.4192 - val_weight_output_acc: 0.6284 - val_bag_output_acc: 0.6505 - val_footwear_output_acc: 0.6420 - val_pose_output_acc: 0.8118 - val_emotion_output_acc: 0.7160\n",
            "Epoch 34/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.1295 - gender_output_loss: 0.2855 - image_quality_output_loss: 0.9270 - age_output_loss: 1.3079 - weight_output_loss: 0.8758 - bag_output_loss: 0.7536 - footwear_output_loss: 0.7505 - pose_output_loss: 0.3867 - emotion_output_loss: 0.8424 - gender_output_acc: 0.8799 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4321 - weight_output_acc: 0.6625 - bag_output_acc: 0.6811 - footwear_output_acc: 0.6672 - pose_output_acc: 0.8509 - emotion_output_acc: 0.7105 - val_loss: 6.5687 - val_gender_output_loss: 0.3546 - val_image_quality_output_loss: 0.9707 - val_age_output_loss: 1.3556 - val_weight_output_loss: 0.9263 - val_bag_output_loss: 0.8129 - val_footwear_output_loss: 0.8048 - val_pose_output_loss: 0.4834 - val_emotion_output_loss: 0.8605 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5503 - val_age_output_acc: 0.4133 - val_weight_output_acc: 0.6247 - val_bag_output_acc: 0.6486 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.8114 - val_emotion_output_acc: 0.7142\n",
            "Epoch 35/60\n",
            "Learning rate: Learning rate:  0.001\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.1151 - gender_output_loss: 0.2814 - image_quality_output_loss: 0.9245 - age_output_loss: 1.3085 - weight_output_loss: 0.8743 - bag_output_loss: 0.7496 - footwear_output_loss: 0.7506 - pose_output_loss: 0.3837 - emotion_output_loss: 0.8426 - gender_output_acc: 0.8807 - image_quality_output_acc: 0.5657 - age_output_acc: 0.4319 - weight_output_acc: 0.6621 - bag_output_acc: 0.6832 - footwear_output_acc: 0.6660 - pose_output_acc: 0.8489 - emotion_output_acc: 0.7105 - val_loss: 6.5826 - val_gender_output_loss: 0.3603 - val_image_quality_output_loss: 0.9706 - val_age_output_loss: 1.3560 - val_weight_output_loss: 0.9257 - val_bag_output_loss: 0.8116 - val_footwear_output_loss: 0.8099 - val_pose_output_loss: 0.4855 - val_emotion_output_loss: 0.8629 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5455 - val_age_output_acc: 0.4118 - val_weight_output_acc: 0.6287 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6427 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7145\n",
            "Epoch 35/60\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.1016 - gender_output_loss: 0.2827 - image_quality_output_loss: 0.9268 - age_output_loss: 1.3096 - weight_output_loss: 0.8780 - bag_output_loss: 0.7466 - footwear_output_loss: 0.7462 - pose_output_loss: 0.3744 - emotion_output_loss: 0.8372 - gender_output_acc: 0.8810 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4279 - weight_output_acc: 0.6569 - bag_output_acc: 0.6852 - footwear_output_acc: 0.6719 - pose_output_acc: 0.8561 - emotion_output_acc: 0.7107 - val_loss: 6.6100 - val_gender_output_loss: 0.3669 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.3591 - val_weight_output_loss: 0.9252 - val_bag_output_loss: 0.8136 - val_footwear_output_loss: 0.8097 - val_pose_output_loss: 0.4917 - val_emotion_output_loss: 0.8659 - val_gender_output_acc: 0.8464 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4118 - val_weight_output_acc: 0.6276 - val_bag_output_acc: 0.6479 - val_footwear_output_acc: 0.6468 - val_pose_output_acc: 0.8085 - val_emotion_output_acc: 0.7134\n",
            "Epoch 37/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.0912 - gender_output_loss: 0.2784 - image_quality_output_loss: 0.9222 - age_output_loss: 1.3058 - weight_output_loss: 0.8738 - bag_output_loss: 0.7452 - footwear_output_loss: 0.7461 - pose_output_loss: 0.3783 - emotion_output_loss: 0.8413 - gender_output_acc: 0.8838 - image_quality_output_acc: 0.5663 - age_output_acc: 0.4276 - weight_output_acc: 0.6584 - bag_output_acc: 0.6872 - footwear_output_acc: 0.6689 - pose_output_acc: 0.8555 - emotion_output_acc: 0.7117 - val_loss: 6.5921 - val_gender_output_loss: 0.3596 - val_image_quality_output_loss: 0.9701 - val_age_output_loss: 1.3571 - val_weight_output_loss: 0.9285 - val_bag_output_loss: 0.8108 - val_footwear_output_loss: 0.8113 - val_pose_output_loss: 0.4922 - val_emotion_output_loss: 0.8626 - val_gender_output_acc: 0.8497 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4099 - val_weight_output_acc: 0.6291 - val_bag_output_acc: 0.6475 - val_footwear_output_acc: 0.6449 - val_pose_output_acc: 0.8018 - val_emotion_output_acc: 0.7145\n",
            "Epoch 38/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 6.0916 - gender_output_loss: 0.2792 - image_quality_output_loss: 0.9224 - age_output_loss: 1.3040 - weight_output_loss: 0.8782 - bag_output_loss: 0.7455 - footwear_output_loss: 0.7403 - pose_output_loss: 0.3820 - emotion_output_loss: 0.8401 - gender_output_acc: 0.8819 - image_quality_output_acc: 0.5685 - age_output_acc: 0.4287 - weight_output_acc: 0.6567 - bag_output_acc: 0.6878 - footwear_output_acc: 0.6760 - pose_output_acc: 0.8498 - emotion_output_acc: 0.7112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.0893 - gender_output_loss: 0.2791 - image_quality_output_loss: 0.9221 - age_output_loss: 1.3033 - weight_output_loss: 0.8779 - bag_output_loss: 0.7451 - footwear_output_loss: 0.7401 - pose_output_loss: 0.3821 - emotion_output_loss: 0.8397 - gender_output_acc: 0.8819 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4291 - weight_output_acc: 0.6568 - bag_output_acc: 0.6880 - footwear_output_acc: 0.6761 - pose_output_acc: 0.8498 - emotion_output_acc: 0.7114 - val_loss: 6.5840 - val_gender_output_loss: 0.3542 - val_image_quality_output_loss: 0.9743 - val_age_output_loss: 1.3657 - val_weight_output_loss: 0.9281 - val_bag_output_loss: 0.8095 - val_footwear_output_loss: 0.8083 - val_pose_output_loss: 0.4815 - val_emotion_output_loss: 0.8623 - val_gender_output_acc: 0.8486 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4096 - val_weight_output_acc: 0.6243 - val_bag_output_acc: 0.6505 - val_footwear_output_acc: 0.6420 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7149\n",
            "Epoch 39/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.0648 - gender_output_loss: 0.2793 - image_quality_output_loss: 0.9193 - age_output_loss: 1.3070 - weight_output_loss: 0.8712 - bag_output_loss: 0.7386 - footwear_output_loss: 0.7353 - pose_output_loss: 0.3784 - emotion_output_loss: 0.8358 - gender_output_acc: 0.8796 - image_quality_output_acc: 0.5704 - age_output_acc: 0.4318 - weight_output_acc: 0.6617 - bag_output_acc: 0.6864 - footwear_output_acc: 0.6765 - pose_output_acc: 0.8527 - emotion_output_acc: 0.7113 - val_loss: 6.5670 - val_gender_output_loss: 0.3556 - val_image_quality_output_loss: 0.9693 - val_age_output_loss: 1.3521 - val_weight_output_loss: 0.9250 - val_bag_output_loss: 0.8098 - val_footwear_output_loss: 0.8079 - val_pose_output_loss: 0.4828 - val_emotion_output_loss: 0.8644 - val_gender_output_acc: 0.8497 - val_image_quality_output_acc: 0.5499 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.6306 - val_bag_output_acc: 0.6497 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.8136 - val_emotion_output_acc: 0.7138\n",
            "Epoch 40/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/60\n",
            "339/340 [============================>.] - ETA: 0s - loss: 6.0536 - gender_output_loss: 0.2748 - image_quality_output_loss: 0.9197 - age_output_loss: 1.3009 - weight_output_loss: 0.8712 - bag_output_loss: 0.7363 - footwear_output_loss: 0.7422 - pose_output_loss: 0.3741 - emotion_output_loss: 0.8344 - gender_output_acc: 0.8836 - image_quality_output_acc: 0.5723 - age_output_acc: 0.4319 - weight_output_acc: 0.6590 - bag_output_acc: 0.6902 - footwear_output_acc: 0.6704 - pose_output_acc: 0.8566 - emotion_output_acc: 0.7117\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.0540 - gender_output_loss: 0.2746 - image_quality_output_loss: 0.9200 - age_output_loss: 1.3012 - weight_output_loss: 0.8706 - bag_output_loss: 0.7371 - footwear_output_loss: 0.7418 - pose_output_loss: 0.3740 - emotion_output_loss: 0.8346 - gender_output_acc: 0.8839 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4316 - weight_output_acc: 0.6592 - bag_output_acc: 0.6899 - footwear_output_acc: 0.6706 - pose_output_acc: 0.8565 - emotion_output_acc: 0.7117 - val_loss: 6.5912 - val_gender_output_loss: 0.3637 - val_image_quality_output_loss: 0.9706 - val_age_output_loss: 1.3593 - val_weight_output_loss: 0.9292 - val_bag_output_loss: 0.8158 - val_footwear_output_loss: 0.8084 - val_pose_output_loss: 0.4812 - val_emotion_output_loss: 0.8630 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.4162 - val_weight_output_acc: 0.6254 - val_bag_output_acc: 0.6486 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.8122 - val_emotion_output_acc: 0.7149\n",
            "Epoch 41/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 6.0501 - gender_output_loss: 0.2749 - image_quality_output_loss: 0.9211 - age_output_loss: 1.2995 - weight_output_loss: 0.8715 - bag_output_loss: 0.7402 - footwear_output_loss: 0.7384 - pose_output_loss: 0.3689 - emotion_output_loss: 0.8356 - gender_output_acc: 0.8825 - image_quality_output_acc: 0.5698 - age_output_acc: 0.4340 - weight_output_acc: 0.6626 - bag_output_acc: 0.6881 - footwear_output_acc: 0.6694 - pose_output_acc: 0.8594 - emotion_output_acc: 0.7113 - val_loss: 6.6071 - val_gender_output_loss: 0.3599 - val_image_quality_output_loss: 0.9733 - val_age_output_loss: 1.3636 - val_weight_output_loss: 0.9286 - val_bag_output_loss: 0.8142 - val_footwear_output_loss: 0.8128 - val_pose_output_loss: 0.4924 - val_emotion_output_loss: 0.8624 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.4125 - val_weight_output_acc: 0.6258 - val_bag_output_acc: 0.6446 - val_footwear_output_acc: 0.6420 - val_pose_output_acc: 0.8081 - val_emotion_output_acc: 0.7160\n",
            "Epoch 42/60\n",
            "\n",
            "Learning rate:  0.001\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.0212 - gender_output_loss: 0.2656 - image_quality_output_loss: 0.9153 - age_output_loss: 1.3004 - weight_output_loss: 0.8660 - bag_output_loss: 0.7381 - footwear_output_loss: 0.7342 - pose_output_loss: 0.3674 - emotion_output_loss: 0.8343 - gender_output_acc: 0.8869 - image_quality_output_acc: 0.5706 - age_output_acc: 0.4276 - weight_output_acc: 0.6633 - bag_output_acc: 0.6919 - footwear_output_acc: 0.6728 - pose_output_acc: 0.8595 - emotion_output_acc: 0.7126 - val_loss: 6.5812 - val_gender_output_loss: 0.3571 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.3569 - val_weight_output_loss: 0.9256 - val_bag_output_loss: 0.8080 - val_footwear_output_loss: 0.8061 - val_pose_output_loss: 0.4896 - val_emotion_output_loss: 0.8631 - val_gender_output_acc: 0.8512 - val_image_quality_output_acc: 0.5389 - val_age_output_acc: 0.4151 - val_weight_output_acc: 0.6276 - val_bag_output_acc: 0.6453 - val_footwear_output_acc: 0.6449 - val_pose_output_acc: 0.8059 - val_emotion_output_acc: 0.7142\n",
            "Epoch 43/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 6.0208 - gender_output_loss: 0.2671 - image_quality_output_loss: 0.9178 - age_output_loss: 1.2968 - weight_output_loss: 0.8640 - bag_output_loss: 0.7358 - footwear_output_loss: 0.7367 - pose_output_loss: 0.3652 - emotion_output_loss: 0.8375 - gender_output_acc: 0.8889 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4361 - weight_output_acc: 0.6626 - bag_output_acc: 0.6943 - footwear_output_acc: 0.6738 - pose_output_acc: 0.8566 - emotion_output_acc: 0.7107 - val_loss: 6.5817 - val_gender_output_loss: 0.3577 - val_image_quality_output_loss: 0.9679 - val_age_output_loss: 1.3604 - val_weight_output_loss: 0.9294 - val_bag_output_loss: 0.8077 - val_footwear_output_loss: 0.8102 - val_pose_output_loss: 0.4857 - val_emotion_output_loss: 0.8627 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4096 - val_weight_output_acc: 0.6221 - val_bag_output_acc: 0.6564 - val_footwear_output_acc: 0.6368 - val_pose_output_acc: 0.8077 - val_emotion_output_acc: 0.7142\n",
            "Epoch 43/60\n",
            " 0.001\n",
            "Epoch 44/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 5.9991 - gender_output_loss: 0.2680 - image_quality_output_loss: 0.9154 - age_output_loss: 1.2938 - weight_output_loss: 0.8633 - bag_output_loss: 0.7338 - footwear_output_loss: 0.7339 - pose_output_loss: 0.3594 - emotion_output_loss: 0.8315 - gender_output_acc: 0.8873 - image_quality_output_acc: 0.5726 - age_output_acc: 0.4283 - weight_output_acc: 0.6607 - bag_output_acc: 0.6994 - footwear_output_acc: 0.6695 - pose_output_acc: 0.8645 - emotion_output_acc: 0.7126 - val_loss: 6.6019 - val_gender_output_loss: 0.3680 - val_image_quality_output_loss: 0.9673 - val_age_output_loss: 1.3593 - val_weight_output_loss: 0.9302 - val_bag_output_loss: 0.8068 - val_footwear_output_loss: 0.8065 - val_pose_output_loss: 0.4990 - val_emotion_output_loss: 0.8648 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5525 - val_age_output_acc: 0.4133 - val_weight_output_acc: 0.6243 - val_bag_output_acc: 0.6545 - val_footwear_output_acc: 0.6475 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7149\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 45/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 6.0047 - gender_output_loss: 0.2636 - image_quality_output_loss: 0.9144 - age_output_loss: 1.2937 - weight_output_loss: 0.8649 - bag_output_loss: 0.7303 - footwear_output_loss: 0.7382 - pose_output_loss: 0.3632 - emotion_output_loss: 0.8364 - gender_output_acc: 0.8899 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4307 - weight_output_acc: 0.6612 - bag_output_acc: 0.6902 - footwear_output_acc: 0.6724 - pose_output_acc: 0.8623 - emotion_output_acc: 0.7119 - val_loss: 6.6013 - val_gender_output_loss: 0.3609 - val_image_quality_output_loss: 0.9685 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9295 - val_bag_output_loss: 0.8091 - val_footwear_output_loss: 0.8101 - val_pose_output_loss: 0.4943 - val_emotion_output_loss: 0.8659 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5503 - val_age_output_acc: 0.4162 - val_weight_output_acc: 0.6273 - val_bag_output_acc: 0.6571 - val_footwear_output_acc: 0.6398 - val_pose_output_acc: 0.8099 - val_emotion_output_acc: 0.7138\n",
            "Epoch 45/60\n",
            "Epoch 46/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 5.9872 - gender_output_loss: 0.2645 - image_quality_output_loss: 0.9103 - age_output_loss: 1.2957 - weight_output_loss: 0.8628 - bag_output_loss: 0.7315 - footwear_output_loss: 0.7243 - pose_output_loss: 0.3627 - emotion_output_loss: 0.8354 - gender_output_acc: 0.8891 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4259 - weight_output_acc: 0.6581 - bag_output_acc: 0.6949 - footwear_output_acc: 0.6802 - pose_output_acc: 0.8607 - emotion_output_acc: 0.7103 - val_loss: 6.6098 - val_gender_output_loss: 0.3611 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.3586 - val_weight_output_loss: 0.9289 - val_bag_output_loss: 0.8113 - val_footwear_output_loss: 0.8158 - val_pose_output_loss: 0.4984 - val_emotion_output_loss: 0.8644 - val_gender_output_acc: 0.8449 - val_image_quality_output_acc: 0.5414 - val_age_output_acc: 0.4107 - val_weight_output_acc: 0.6269 - val_bag_output_acc: 0.6512 - val_footwear_output_acc: 0.6350 - val_pose_output_acc: 0.8033 - val_emotion_output_acc: 0.7149\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/60Learning rate: Epoch 47/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 5.9772 - gender_output_loss: 0.2654 - image_quality_output_loss: 0.9121 - age_output_loss: 1.2883 - weight_output_loss: 0.8620 - bag_output_loss: 0.7300 - footwear_output_loss: 0.7267 - pose_output_loss: 0.3583 - emotion_output_loss: 0.8344 - gender_output_acc: 0.8896 - image_quality_output_acc: 0.5754 - age_output_acc: 0.4363 - weight_output_acc: 0.6596 - bag_output_acc: 0.6950 - footwear_output_acc: 0.6809 - pose_output_acc: 0.8607 - emotion_output_acc: 0.7111\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 5.9772 - gender_output_loss: 0.2657 - image_quality_output_loss: 0.9119 - age_output_loss: 1.2884 - weight_output_loss: 0.8622 - bag_output_loss: 0.7297 - footwear_output_loss: 0.7269 - pose_output_loss: 0.3584 - emotion_output_loss: 0.8340 - gender_output_acc: 0.8896 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4365 - weight_output_acc: 0.6592 - bag_output_acc: 0.6952 - footwear_output_acc: 0.6810 - pose_output_acc: 0.8606 - emotion_output_acc: 0.7115 - val_loss: 6.5955 - val_gender_output_loss: 0.3617 - val_image_quality_output_loss: 0.9690 - val_age_output_loss: 1.3601 - val_weight_output_loss: 0.9278 - val_bag_output_loss: 0.8091 - val_footwear_output_loss: 0.8092 - val_pose_output_loss: 0.4940 - val_emotion_output_loss: 0.8646 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5440 - val_age_output_acc: 0.4140 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6490 - val_footwear_output_acc: 0.6405 - val_pose_output_acc: 0.8103 - val_emotion_output_acc: 0.7131\n",
            "Epoch 48/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
            "339/340 [============================>.] - ETA: 0s - loss: 5.9638 - gender_output_loss: 0.2618 - image_quality_output_loss: 0.9100 - age_output_loss: 1.2905 - weight_output_loss: 0.8617 - bag_output_loss: 0.7289 - footwear_output_loss: 0.7239 - pose_output_loss: 0.3556 - emotion_output_loss: 0.8314 - gender_output_acc: 0.8890 - image_quality_output_acc: 0.5715 - age_output_acc: 0.4330 - weight_output_acc: 0.6598 - bag_output_acc: 0.6967 - footwear_output_acc: 0.6777 - pose_output_acc: 0.8607 - emotion_output_acc: 0.7112\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 5.9635 - gender_output_loss: 0.2618 - image_quality_output_loss: 0.9099 - age_output_loss: 1.2907 - weight_output_loss: 0.8618 - bag_output_loss: 0.7287 - footwear_output_loss: 0.7239 - pose_output_loss: 0.3557 - emotion_output_loss: 0.8310 - gender_output_acc: 0.8890 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4330 - weight_output_acc: 0.6597 - bag_output_acc: 0.6968 - footwear_output_acc: 0.6779 - pose_output_acc: 0.8607 - emotion_output_acc: 0.7113 - val_loss: 6.5975 - val_gender_output_loss: 0.3565 - val_image_quality_output_loss: 0.9665 - val_age_output_loss: 1.3674 - val_weight_output_loss: 0.9275 - val_bag_output_loss: 0.8050 - val_footwear_output_loss: 0.8162 - val_pose_output_loss: 0.4907 - val_emotion_output_loss: 0.8678 - val_gender_output_acc: 0.8475 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.4063 - val_weight_output_acc: 0.6214 - val_bag_output_acc: 0.6582 - val_footwear_output_acc: 0.6413 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.7134\n",
            "Epoch 49/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 5.9535 - gender_output_loss: 0.2543 - image_quality_output_loss: 0.9102 - age_output_loss: 1.2897 - weight_output_loss: 0.8625 - bag_output_loss: 0.7246 - footwear_output_loss: 0.7231 - pose_output_loss: 0.3591 - emotion_output_loss: 0.8300 - gender_output_acc: 0.8926 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4324 - weight_output_acc: 0.6611 - bag_output_acc: 0.6943 - footwear_output_acc: 0.6787 - pose_output_acc: 0.8626 - emotion_output_acc: 0.7118 - val_loss: 6.5890 - val_gender_output_loss: 0.3574 - val_image_quality_output_loss: 0.9715 - val_age_output_loss: 1.3517 - val_weight_output_loss: 0.9225 - val_bag_output_loss: 0.8104 - val_footwear_output_loss: 0.8131 - val_pose_output_loss: 0.4934 - val_emotion_output_loss: 0.8689 - val_gender_output_acc: 0.8442 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6324 - val_bag_output_acc: 0.6534 - val_footwear_output_acc: 0.6464 - val_pose_output_acc: 0.8096 - val_emotion_output_acc: 0.7145\n",
            "Epoch 50/60\n",
            "Learning rate: Learning rate:  0.001\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 5.9324 - gender_output_loss: 0.2537 - image_quality_output_loss: 0.9083 - age_output_loss: 1.2838 - weight_output_loss: 0.8597 - bag_output_loss: 0.7275 - footwear_output_loss: 0.7242 - pose_output_loss: 0.3462 - emotion_output_loss: 0.8288 - gender_output_acc: 0.8954 - image_quality_output_acc: 0.5754 - age_output_acc: 0.4371 - weight_output_acc: 0.6613 - bag_output_acc: 0.6997 - footwear_output_acc: 0.6795 - pose_output_acc: 0.8679 - emotion_output_acc: 0.7125 - val_loss: 6.6096 - val_gender_output_loss: 0.3665 - val_image_quality_output_loss: 0.9672 - val_age_output_loss: 1.3629 - val_weight_output_loss: 0.9301 - val_bag_output_loss: 0.8115 - val_footwear_output_loss: 0.8132 - val_pose_output_loss: 0.4893 - val_emotion_output_loss: 0.8689 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4114 - val_weight_output_acc: 0.6232 - val_bag_output_acc: 0.6541 - val_footwear_output_acc: 0.6383 - val_pose_output_acc: 0.8114 - val_emotion_output_acc: 0.7134\n",
            "Epoch 51/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 118ms/step - loss: 5.9229 - gender_output_loss: 0.2523 - image_quality_output_loss: 0.9043 - age_output_loss: 1.2816 - weight_output_loss: 0.8575 - bag_output_loss: 0.7253 - footwear_output_loss: 0.7270 - pose_output_loss: 0.3481 - emotion_output_loss: 0.8267 - gender_output_acc: 0.8937 - image_quality_output_acc: 0.5748 - age_output_acc: 0.4385 - weight_output_acc: 0.6603 - bag_output_acc: 0.6952 - footwear_output_acc: 0.6780 - pose_output_acc: 0.8687 - emotion_output_acc: 0.7115 - val_loss: 6.5998 - val_gender_output_loss: 0.3662 - val_image_quality_output_loss: 0.9679 - val_age_output_loss: 1.3597 - val_weight_output_loss: 0.9250 - val_bag_output_loss: 0.8074 - val_footwear_output_loss: 0.8108 - val_pose_output_loss: 0.4943 - val_emotion_output_loss: 0.8685 - val_gender_output_acc: 0.8464 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.4103 - val_weight_output_acc: 0.6287 - val_bag_output_acc: 0.6578 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.8147 - val_emotion_output_acc: 0.7142\n",
            "Epoch 52/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 5.9085 - gender_output_loss: 0.2480 - image_quality_output_loss: 0.9038 - age_output_loss: 1.2807 - weight_output_loss: 0.8515 - bag_output_loss: 0.7239 - footwear_output_loss: 0.7214 - pose_output_loss: 0.3470 - emotion_output_loss: 0.8322 - gender_output_acc: 0.8969 - image_quality_output_acc: 0.5759 - age_output_acc: 0.4350 - weight_output_acc: 0.6657 - bag_output_acc: 0.6974 - footwear_output_acc: 0.6805 - pose_output_acc: 0.8664 - emotion_output_acc: 0.7111 - val_loss: 6.6188 - val_gender_output_loss: 0.3672 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.3619 - val_weight_output_loss: 0.9316 - val_bag_output_loss: 0.8116 - val_footwear_output_loss: 0.8127 - val_pose_output_loss: 0.4889 - val_emotion_output_loss: 0.8689 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.4144 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.6593 - val_footwear_output_acc: 0.6464 - val_pose_output_acc: 0.8103 - val_emotion_output_acc: 0.7142\n",
            "Epoch 52/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 53/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 5.9100 - gender_output_loss: 0.2496 - image_quality_output_loss: 0.9079 - age_output_loss: 1.2822 - weight_output_loss: 0.8540 - bag_output_loss: 0.7188 - footwear_output_loss: 0.7224 - pose_output_loss: 0.3474 - emotion_output_loss: 0.8277 - gender_output_acc: 0.8936 - image_quality_output_acc: 0.5735 - age_output_acc: 0.4344 - weight_output_acc: 0.6627 - bag_output_acc: 0.7003 - footwear_output_acc: 0.6801 - pose_output_acc: 0.8668 - emotion_output_acc: 0.7118 - val_loss: 6.6120 - val_gender_output_loss: 0.3629 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.3656 - val_weight_output_loss: 0.9318 - val_bag_output_loss: 0.8091 - val_footwear_output_loss: 0.8105 - val_pose_output_loss: 0.4960 - val_emotion_output_loss: 0.8705 - val_gender_output_acc: 0.8486 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.4000 - val_weight_output_acc: 0.6217 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6424 - val_pose_output_acc: 0.8096 - val_emotion_output_acc: 0.7142\n",
            "Learning rate:  0.001\n",
            "Epoch 54/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 5.9053 - gender_output_loss: 0.2455 - image_quality_output_loss: 0.9067 - age_output_loss: 1.2799 - weight_output_loss: 0.8546 - bag_output_loss: 0.7161 - footwear_output_loss: 0.7219 - pose_output_loss: 0.3519 - emotion_output_loss: 0.8286 - gender_output_acc: 0.8976 - image_quality_output_acc: 0.5739 - age_output_acc: 0.4448 - weight_output_acc: 0.6638 - bag_output_acc: 0.7012 - footwear_output_acc: 0.6803 - pose_output_acc: 0.8667 - emotion_output_acc: 0.7130 - val_loss: 6.6391 - val_gender_output_loss: 0.3739 - val_image_quality_output_loss: 0.9659 - val_age_output_loss: 1.3625 - val_weight_output_loss: 0.9353 - val_bag_output_loss: 0.8153 - val_footwear_output_loss: 0.8209 - val_pose_output_loss: 0.4957 - val_emotion_output_loss: 0.8696 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.4114 - val_weight_output_acc: 0.6188 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6409 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7145\n",
            "Epoch 55/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 5.8818 - gender_output_loss: 0.2516 - image_quality_output_loss: 0.9039 - age_output_loss: 1.2770 - weight_output_loss: 0.8465 - bag_output_loss: 0.7212 - footwear_output_loss: 0.7120 - pose_output_loss: 0.3402 - emotion_output_loss: 0.8293 - gender_output_acc: 0.8977 - image_quality_output_acc: 0.5773 - age_output_acc: 0.4367 - weight_output_acc: 0.6649 - bag_output_acc: 0.6994 - footwear_output_acc: 0.6822 - pose_output_acc: 0.8697 - emotion_output_acc: 0.7108 - val_loss: 6.6419 - val_gender_output_loss: 0.3770 - val_image_quality_output_loss: 0.9691 - val_age_output_loss: 1.3606 - val_weight_output_loss: 0.9329 - val_bag_output_loss: 0.8157 - val_footwear_output_loss: 0.8164 - val_pose_output_loss: 0.4968 - val_emotion_output_loss: 0.8734 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4063 - val_weight_output_acc: 0.6221 - val_bag_output_acc: 0.6530 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.8125 - val_emotion_output_acc: 0.7134\n",
            "Epoch 56/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 55/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 5.8666 - gender_output_loss: 0.2356 - image_quality_output_loss: 0.9048 - age_output_loss: 1.2741 - weight_output_loss: 0.8476 - bag_output_loss: 0.7150 - footwear_output_loss: 0.7200 - pose_output_loss: 0.3435 - emotion_output_loss: 0.8261 - gender_output_acc: 0.9038 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4370 - weight_output_acc: 0.6664 - bag_output_acc: 0.6990 - footwear_output_acc: 0.6794 - pose_output_acc: 0.8697 - emotion_output_acc: 0.7116 - val_loss: 6.6377 - val_gender_output_loss: 0.3705 - val_image_quality_output_loss: 0.9721 - val_age_output_loss: 1.3688 - val_weight_output_loss: 0.9332 - val_bag_output_loss: 0.8164 - val_footwear_output_loss: 0.8159 - val_pose_output_loss: 0.4928 - val_emotion_output_loss: 0.8680 - val_gender_output_acc: 0.8483 - val_image_quality_output_acc: 0.5429 - val_age_output_acc: 0.4085 - val_weight_output_acc: 0.6239 - val_bag_output_acc: 0.6578 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.8081 - val_emotion_output_acc: 0.7149\n",
            "Epoch 57/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 5.8628 - gender_output_loss: 0.2421 - image_quality_output_loss: 0.8971 - age_output_loss: 1.2732 - weight_output_loss: 0.8505 - bag_output_loss: 0.7155 - footwear_output_loss: 0.7134 - pose_output_loss: 0.3386 - emotion_output_loss: 0.8322 - gender_output_acc: 0.8973 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4410 - weight_output_acc: 0.6623 - bag_output_acc: 0.7014 - footwear_output_acc: 0.6845 - pose_output_acc: 0.8710 - emotion_output_acc: 0.7123 - val_loss: 6.6196 - val_gender_output_loss: 0.3625 - val_image_quality_output_loss: 0.9693 - val_age_output_loss: 1.3624 - val_weight_output_loss: 0.9308 - val_bag_output_loss: 0.8139 - val_footwear_output_loss: 0.8182 - val_pose_output_loss: 0.4945 - val_emotion_output_loss: 0.8681 - val_gender_output_acc: 0.8501 - val_image_quality_output_acc: 0.5484 - val_age_output_acc: 0.4110 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6534 - val_footwear_output_acc: 0.6398 - val_pose_output_acc: 0.8088 - val_emotion_output_acc: 0.7160\n",
            "Epoch 58/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 40s 119ms/step - loss: 5.8554 - gender_output_loss: 0.2462 - image_quality_output_loss: 0.8998 - age_output_loss: 1.2720 - weight_output_loss: 0.8398 - bag_output_loss: 0.7171 - footwear_output_loss: 0.7138 - pose_output_loss: 0.3376 - emotion_output_loss: 0.8290 - gender_output_acc: 0.8984 - image_quality_output_acc: 0.5770 - age_output_acc: 0.4444 - weight_output_acc: 0.6709 - bag_output_acc: 0.7017 - footwear_output_acc: 0.6853 - pose_output_acc: 0.8698 - emotion_output_acc: 0.7118 - val_loss: 6.5988 - val_gender_output_loss: 0.3580 - val_image_quality_output_loss: 0.9668 - val_age_output_loss: 1.3633 - val_weight_output_loss: 0.9323 - val_bag_output_loss: 0.8099 - val_footwear_output_loss: 0.8143 - val_pose_output_loss: 0.4876 - val_emotion_output_loss: 0.8666 - val_gender_output_acc: 0.8501 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.4114 - val_weight_output_acc: 0.6239 - val_bag_output_acc: 0.6541 - val_footwear_output_acc: 0.6398 - val_pose_output_acc: 0.8122 - val_emotion_output_acc: 0.7153\n",
            "\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.Epoch 59/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
            "340/340 [==============================] - 41s 119ms/step - loss: 5.8473 - gender_output_loss: 0.2434 - image_quality_output_loss: 0.8959 - age_output_loss: 1.2753 - weight_output_loss: 0.8457 - bag_output_loss: 0.7153 - footwear_output_loss: 0.7080 - pose_output_loss: 0.3344 - emotion_output_loss: 0.8293 - gender_output_acc: 0.8969 - image_quality_output_acc: 0.5794 - age_output_acc: 0.4422 - weight_output_acc: 0.6670 - bag_output_acc: 0.7047 - footwear_output_acc: 0.6905 - pose_output_acc: 0.8709 - emotion_output_acc: 0.7112 - val_loss: 6.6455 - val_gender_output_loss: 0.3690 - val_image_quality_output_loss: 0.9644 - val_age_output_loss: 1.3708 - val_weight_output_loss: 0.9372 - val_bag_output_loss: 0.8177 - val_footwear_output_loss: 0.8201 - val_pose_output_loss: 0.4979 - val_emotion_output_loss: 0.8685 - val_gender_output_acc: 0.8479 - val_image_quality_output_acc: 0.5429 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6368 - val_pose_output_acc: 0.8026 - val_emotion_output_acc: 0.7153\n",
            "Epoch 60/60\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 59/60\n",
            "340/340 [==============================] - 41s 120ms/step - loss: 5.8610 - gender_output_loss: 0.2389 - image_quality_output_loss: 0.9001 - age_output_loss: 1.2770 - weight_output_loss: 0.8439 - bag_output_loss: 0.7132 - footwear_output_loss: 0.7178 - pose_output_loss: 0.3430 - emotion_output_loss: 0.8271 - gender_output_acc: 0.9001 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4363 - weight_output_acc: 0.6629 - bag_output_acc: 0.7030 - footwear_output_acc: 0.6831 - pose_output_acc: 0.8650 - emotion_output_acc: 0.7114 - val_loss: 6.6184 - val_gender_output_loss: 0.3653 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.3635 - val_weight_output_loss: 0.9323 - val_bag_output_loss: 0.8144 - val_footwear_output_loss: 0.8176 - val_pose_output_loss: 0.4934 - val_emotion_output_loss: 0.8694 - val_gender_output_acc: 0.8505 - val_image_quality_output_acc: 0.5484 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6541 - val_footwear_output_acc: 0.6383 - val_pose_output_acc: 0.8103 - val_emotion_output_acc: 0.7145\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79984287b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}